% \chapter{Methodology}
% \label{ch:method}

% This chapter provides a comprehensive explanation of the methods employed in our research to assess the proficiency of ChatGPT in solving data structures and algorithms (DSA) coding problems using role-specific prompts. The methodology follows a systematic process of data collection, data preparation, prompt engineering, solution processing, automated testing, and performance analysis.

% \section{Data Collection}
% \subsection{Fetching LeetCode Questions}
% Instead of using traditional web scraping techniques, our project utilized the LeetCode GraphQL API to fetch coding questions directly from LeetCode. This approach allowed for a more efficient and reliable extraction of data, including question texts, metadata, code stubs, and public test cases. The dataset comprises a total of 1,689 questions, categorized as follows: 422 easy, 855 medium, and 412 hard questions.

% \subsection{Dataset Preparation}
% From the collected data, a subset of questions was selected based on criteria designed to evaluate the effectiveness of ChatGPT across varying difficulties. The subset includes 133 easy, 312 medium, and 124 hard questions. This selection aims to balance the representation of different difficulty levels in the subsequent analysis.

% \section{Prompt Engineering}
% \subsection{Design and Implementation of Role-Specific Prompts}
% Role-specific prompts were meticulously designed to instruct ChatGPT to approach the problems from various theoretical perspectives and practical scenarios. A total of nine different prompts were developed, each tailored to evoke a distinct approach or methodology in solving the coding tasks. These prompts were crafted based on insights drawn from the literature review, focusing on enhancing the model's context understanding and response relevance.

% \subsection{Interaction with OpenAI's ChatGPT-3.5 Turbo}
% For each selected problem, the role-specific prompts were deployed via the OpenAI API to interact with ChatGPT-3.5 Turbo. The API facilitated the automation of sending prompts and receiving ChatGPT’s code solutions, ensuring a streamlined process for large-scale testing.

% \section{Solution Processing and Testing}
% \subsection{Automated Solution Adjustment}
% Received solutions from ChatGPT were automatically processed through scripts that perform essential adjustments. These adjustments included the removal of non-essential strings and docstrings and the addition of missing import statements, which are crucial for the correct execution of the Python code.

% \subsection{Automated Testing Using Pytest}
% The adjusted solutions were then tested using Pytest, an automated testing framework. This step is critical to assess the functional correctness of the solutions against the predefined public test cases provided with the LeetCode questions.

% \section{Performance Analysis}
% \subsection{Data Aggregation}
% After testing, results were compiled into CSV files for each question and corresponding prompt. These files include detailed metadata such as the number of tests passed, failed, and any runtime errors, providing a granular view of performance across different prompts and question difficulties.

% \subsection{Comprehensive Analysis}
% The performance data, combined with the initially scraped metadata, were analyzed to evaluate the impact of prompt specificity on ChatGPT’s effectiveness in coding tasks. This analysis aims to identify patterns and derive insights on the optimal use of role-based prompts in programming problem-solving contexts.

% \section{Additional Resources}
% For further details, including the source code and data processing scripts, the project repository is available at \href{https://github.com/csci595-research-lit-spring-2024/595-class-project-spring-2024-Mokshithy/tree/main/src/src}{GitHub}.

% \section{Summary}
% This methodology chapter has delineated the structured approach utilized in this study to systematically evaluate the capabilities of ChatGPT in solving DSA challenges through the lens of role-specific prompts. Each phase of the methodology—from data collection to detailed performance analysis—was crafted to ensure a comprehensive assessment of the AI model’s coding proficiency, thereby contributing to the broader academic discourse on enhancing AI's utility in complex problem-solving scenarios.


\chapter{Methodology}
\label{ch:method}

This chapter details the comprehensive methodology employed in our research to assess the proficiency of ChatGPT 3.5 turbo, a state-of-the-art language model developed by OpenAI, in solving data structures and algorithms (DSA) coding problems using a variety of role-specific prompts. The process encompasses several phases including data collection, prompt engineering, solution processing, automated testing, and performance analysis, each designed to scrutinize the effectiveness of ChatGPT under different coding contexts.

\section{Context and Objectives}
The primary objective of this study is to create a streamlined and automated pipeline to explore how effectively ChatGPT can address DSA coding problems when guided by role-specific prompts that simulate different user roles or coding scenarios. This involves evaluating the model's ability to understand and generate appropriate code responses that successfully solve given problems. The study leverages a subset of problems from LeetCode, as mentioned in \ref{sec:Leetcode}.

\section{Data Collection}
\subsection{Fetching LeetCode Questions}
Our data collection process deviates from traditional web scraping techniques. Instead, we utilized the LeetCode GraphQL API to programmatically fetch coding questions. This method provided a more stable and efficient means of extracting data, which includes question texts, metadata, code stubs, and public test cases. The dataset includes a total of 1,689 questions, categorized into 422 easy, 855 medium, and 412 hard questions.

\subsection{Dataset Preparation}
From the extensive pool of questions, we selected a balanced subset to ensure a comprehensive evaluation across various difficulty levels. The subset comprises 133 easy, 312 medium, and 124 hard questions. This selection was strategically made to challenge ChatGPT's coding capabilities under varying levels of complexity.

\section{Prompt Engineering}
\subsection{Design and Implementation of Role-Specific Prompts}
To simulate different coding scenarios and user roles, we designed nine distinct prompts. Each prompt was crafted to guide ChatGPT's response towards specific problem-solving strategies or viewpoints. These prompts are critical in evaluating how variations in query formulation can influence the effectiveness and accuracy of the solutions provided by ChatGPT.

\subsection{Interaction with OpenAI's ChatGPT-3.5 Turbo}
Each selected coding problem, accompanied by one of the nine role-specific prompts, was presented to ChatGPT-3.5 Turbo via the OpenAI API. This interaction was automated to handle the high volume of test cases, ensuring that each problem was processed efficiently and consistently.

\section{Solution Processing and Testing}
\subsection{Automated Solution Adjustment}
Upon receiving solutions from ChatGPT, a series of automated scripts were employed to refine the outputs. This refinement process included the removal of extraneous strings and docstrings, and the addition of necessary Python import statements to ensure that the code could be executed correctly in a Python environment.

\subsection{Automated Testing Using Pytest}
Following adjustment, solutions were automatically tested using Pytest, a powerful testing framework that verifies the functional correctness of code against the predefined test cases provided with the LeetCode problems. This testing phase is essential for objectively assessing the accuracy of the solutions generated by ChatGPT.

\section{Performance Analysis}
\subsection{Data Aggregation}
Results from the testing phase were systematically compiled into CSV files for each problem and corresponding prompt. These files contain detailed information including the number of tests passed, failed, and any errors encountered, providing a detailed breakdown of performance. The key metric observed in the study is acceptance rate which indicates whether all the test cases are passed without any errors. 

\subsection{Comprehensive Analysis}
The aggregated performance data, along with the metadata initially scraped, were thoroughly analyzed to assess the impact of prompt specificity on ChatGPT’s coding performance. This analysis focuses on identifying trends and deriving insights that could help optimize the use of AI for solving complex coding problems.

\section{Summary}
The methodology outlined in this chapter establishes a detailed and automated framework for evaluating the capabilities of ChatGPT in the realm of DSA problem-solving through targeted prompt engineering. By systematically exploring the interaction between the AI model and structured coding prompts, our study aims to contribute to the ongoing development of AI tools capable of effectively assisting in complex software development tasks.
