@article{radford2019language,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},
  journal={OpenAI blog},
  volume={1},
  number={8},
  pages={9},
  year={2019}
}

@article{chen2021evaluating,
  title={Evaluating large language models trained on code},
  author={Chen, Mark and Tworek, Jerry and Jun, Heewoo and Yuan, Qiming and Pinto, Henrique Ponde de Oliveira and Kaplan, Jared and Edwards, Harri and Burda, Yuri and Joseph, Nicholas and Brockman, Greg and others},
  journal={arXiv preprint arXiv:2107.03374},
  year={2021}
}

@misc{arefin2023unmasking,
      title={Unmasking the giant: A comprehensive evaluation of ChatGPT's proficiency in coding algorithms and data structures}, 
      author={Sayed Erfan Arefin and Tasnia Ashrafi Heya and Hasan Al-Qudah and Ynes Ineza and Abdul Serwadda},
      year={2023},
      eprint={2307.05360},
      archivePrefix={arXiv},
      primaryClass={cs.SE}
}

@misc{noever2023numeracy,
      title={Numeracy from Literacy: Data Science as an Emergent Skill from Large Language Models}, 
      author={David Noever and Forrest McKee},
      year={2023},
      eprint={2301.13382},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{Biswas,
author = {Biswas, Som},
year = {2023},
month = {03},
pages = {},
title = {Role of ChatGPT in Computer Programming},
doi = {10.58496/MJCSC/2023/002}
}

@misc{bubeck2023sparks,
      title={Sparks of Artificial General Intelligence: Early experiments with GPT-4}, 
      author={Sébastien Bubeck and Varun Chandrasekaran and Ronen Eldan and Johannes Gehrke and Eric Horvitz and Ece Kamar and Peter Lee and Yin Tat Lee and Yuanzhi Li and Scott Lundberg and Harsha Nori and Hamid Palangi and Marco Tulio Ribeiro and Yi Zhang},
      year={2023},
      eprint={2303.12712},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{tian2023chatgpt,
      title={Is ChatGPT the Ultimate Programming Assistant -- How far is it?}, 
      author={Haoye Tian and Weiqi Lu and Tsz On Li and Xunzhu Tang and Shing-Chi Cheung and Jacques Klein and Tegawendé F. Bissyandé},
      year={2023},
      eprint={2304.11938},
      archivePrefix={arXiv},
      primaryClass={cs.SE}
}

@INPROCEEDINGS{10196869,
  author={Feng, Yunhe and Vanam, Sreecharan and Cherukupally, Manasa and Zheng, Weijian and Qiu, Meikang and Chen, Haihua},
  booktitle={2023 IEEE 47th Annual Computers, Software, and Applications Conference (COMPSAC)}, 
  title={Investigating Code Generation Performance of ChatGPT with Crowdsourcing Social Data}, 
  year={2023},
  volume={},
  number={},
  pages={876-885},
  keywords={Codes;Social networking (online);Debugging;Chatbots;Software;Task analysis;Interviews;ChatGPT;Coding Generation;Software Engineering;Large Language Models (LLMs);Generative Models;Social Media},
  doi={10.1109/COMPSAC57700.2023.00117}}

@Article{info:doi/10.2196/45312,
author="Gilson, Aidan
and Safranek, Conrad W
and Huang, Thomas
and Socrates, Vimig
and Chi, Ling
and Taylor, Richard Andrew
and Chartash, David",
title="How Does ChatGPT Perform on the United States Medical Licensing Examination? The Implications of Large Language Models for Medical Education and Knowledge Assessment",
journal="JMIR Med Educ",
year="2023",
month="Feb",
day="8",
volume="9",
pages="e45312",
keywords="natural language processing; NLP; MedQA; generative pre-trained transformer; GPT; medical education; chatbot; artificial intelligence; education technology; ChatGPT; conversational agent; machine learning",
abstract="Background: Chat Generative Pre-trained Transformer (ChatGPT) is a 175-billion-parameter natural language processing model that can generate conversation-style responses to user input. Objective: This study aimed to evaluate the performance of ChatGPT on questions within the scope of the United States Medical Licensing Examination Step 1 and Step 2 exams, as well as to analyze responses for user interpretability. Methods: We used 2 sets of multiple-choice questions to evaluate ChatGPT's performance, each with questions pertaining to Step 1 and Step 2. The first set was derived from AMBOSS, a commonly used question bank for medical students, which also provides statistics on question difficulty and the performance on an exam relative to the user base. The second set was the National Board of Medical Examiners (NBME) free 120 questions. ChatGPT's performance was compared to 2 other large language models, GPT-3 and InstructGPT. The text output of each ChatGPT response was evaluated across 3 qualitative metrics: logical justification of the answer selected, presence of information internal to the question, and presence of information external to the question. Results: Of the 4 data sets, AMBOSS-Step1, AMBOSS-Step2, NBME-Free-Step1, and NBME-Free-Step2, ChatGPT achieved accuracies of 44{\%} (44/100), 42{\%} (42/100), 64.4{\%} (56/87), and 57.8{\%} (59/102), respectively. ChatGPT outperformed InstructGPT by 8.15{\%} on average across all data sets, and GPT-3 performed similarly to random chance. The model demonstrated a significant decrease in performance as question difficulty increased (P=.01) within the AMBOSS-Step1 data set. We found that logical justification for ChatGPT's answer selection was present in 100{\%} of outputs of the NBME data sets. Internal information to the question was present in 96.8{\%} (183/189) of all questions. The presence of information external to the question was 44.5{\%} and 27{\%} lower for incorrect answers relative to correct answers on the NBME-Free-Step1 (P<.001) and NBME-Free-Step2 (P=.001) data sets, respectively. Conclusions: ChatGPT marks a significant improvement in natural language processing models on the tasks of medical question answering. By performing at a greater than 60{\%} threshold on the NBME-Free-Step-1 data set, we show that the model achieves the equivalent of a passing score for a third-year medical student. Additionally, we highlight ChatGPT's capacity to provide logic and informational context across the majority of answers. These facts taken together make a compelling case for the potential applications of ChatGPT as an interactive medical education tool to support learning. ",
issn="2369-3762",
doi="10.2196/45312",
url="https://mededu.jmir.org/2023/1/e45312",
url="https://doi.org/10.2196/45312",
url="http://www.ncbi.nlm.nih.gov/pubmed/36753318"
}

@misc{frieder2023mathematical,
      title={Mathematical Capabilities of ChatGPT}, 
      author={Simon Frieder and Luca Pinchetti and Alexis Chevalier and Ryan-Rhys Griffiths and Tommaso Salvatori and Thomas Lukasiewicz and Philipp Christian Petersen and Julius Berner},
      year={2023},
      eprint={2301.13867},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{zhuo2023red,
      title={Red teaming ChatGPT via Jailbreaking: Bias, Robustness, Reliability and Toxicity}, 
      author={Terry Yue Zhuo and Yujin Huang and Chunyang Chen and Zhenchang Xing},
      year={2023},
      eprint={2301.12867},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}